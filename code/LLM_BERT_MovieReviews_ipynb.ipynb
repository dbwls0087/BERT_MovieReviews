{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbwls0087/BERT_MovieReviews/blob/main/code/LLM_BERT_MovieReviews_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n"
      ],
      "metadata": {
        "id": "zWDdXdks7_PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsI6ByPq4iTh"
      },
      "outputs": [],
      "source": [
        "csv_path = \"/content/IMDB Dataset.csv\"\n",
        "model_name = \"bert-base-uncased\"\n",
        "max_len = 128\n",
        "batch_size = 16\n",
        "epochs = 1\n",
        "lr = 2e-5\n",
        "save_dir = \"/content/imdb_bert_sentiment\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing"
      ],
      "metadata": {
        "id": "blJ06YdZbK2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(csv_path).dropna(subset=[\"review\", \"sentiment\"]).copy()\n",
        "\n",
        "label_map = {\"negative\": 0, \"positive\": 1}\n",
        "df[\"label\"] = df[\"sentiment\"].map(label_map).astype(int)\n",
        "\n",
        "train_df, valid_df = train_test_split(\n",
        "    df[[\"review\", \"label\"]],\n",
        "    test_size=0.1,\n",
        "    random_state=42,\n",
        "    stratify=df[\"label\"]\n",
        ")\n",
        "\n",
        "print(len(train_df), len(valid_df))"
      ],
      "metadata": {
        "id": "ImDsqctT8rTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "class imdb_dataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.texts = dataframe[\"review\"].astype(str).tolist()\n",
        "        self.labels = dataframe[\"label\"].astype(int).tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
        "        }\n",
        "\n",
        "train_ds = imdb_dataset(train_df, tokenizer, max_len)\n",
        "valid_ds = imdb_dataset(valid_df, tokenizer, max_len)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "aufhXaSw81xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model"
      ],
      "metadata": {
        "id": "gY52x3xZt7Co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "t8XFbga386o5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch():\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n"
      ],
      "metadata": {
        "id": "bSovOIKS8-Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def eval_accuracy():\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in valid_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].cpu().numpy()\n",
        "\n",
        "        logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "\n",
        "        all_preds.append(preds)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    return accuracy_score(all_labels, all_preds)"
      ],
      "metadata": {
        "id": "vBbMYvuB9AO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train_one_epoch()\n",
        "    val_acc = eval_accuracy()\n",
        "    print(f\"epoch {epoch} | train_loss {train_loss:.4f} | val_acc {val_acc:.4f}\")\n",
        "\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "print(\"saved:\", save_dir)"
      ],
      "metadata": {
        "id": "WG5fgV3q9Ce6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## inference"
      ],
      "metadata": {
        "id": "uUAJlDkohkBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def infer(texts):\n",
        "    tok = AutoTokenizer.from_pretrained(save_dir)\n",
        "    mdl = AutoModelForSequenceClassification.from_pretrained(save_dir).to(device)\n",
        "    mdl.eval()\n",
        "\n",
        "    enc = tok(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    logits = mdl(\n",
        "        input_ids=enc[\"input_ids\"].to(device),\n",
        "        attention_mask=enc[\"attention_mask\"].to(device)\n",
        "    ).logits\n",
        "\n",
        "    probs = torch.softmax(logits, dim=-1)[:, 1].detach().cpu().numpy()\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "    return probs, preds\n",
        "\n",
        "probs, preds = infer([\"this movie was amazing\", \"worst movie ever\"])\n",
        "print(\"probs:\", probs)\n",
        "print(\"preds(1=positive):\", preds)\n"
      ],
      "metadata": {
        "id": "kZGAlQk-9Dt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def predict_labels(texts, batch_size = 16):\n",
        "    tok = AutoTokenizer.from_pretrained(save_dir)\n",
        "    mdl = AutoModelForSequenceClassification.from_pretrained(save_dir).to(device)\n",
        "    mdl.eval()\n",
        "\n",
        "    preds_all = []\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "\n",
        "        enc = tok(batch_texts, truncation=True, padding=True, max_length=max_len, return_tensors=\"pt\")\n",
        "        logits = mdl(\n",
        "            input_ids=enc[\"input_ids\"].to(device),\n",
        "            attention_mask=enc[\"attention_mask\"].to(device)\n",
        "        ).logits\n",
        "\n",
        "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        preds_all.append(preds)\n",
        "\n",
        "\n",
        "    return np.concatenate(preds_all)"
      ],
      "metadata": {
        "id": "ujuhsCBT1z6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = pd.read_csv(csv_path).dropna(subset=[\"review\"]).copy()\n",
        "texts = df_all[\"review\"].astype(str).tolist()\n",
        "\n",
        "df_all[\"pred_label\"] = predict_labels(texts)\n",
        "df_all[\"pred_sentiment\"] = np.where(df_all[\"pred_label\"] == 1, \"positive\", \"negative\")\n",
        "\n",
        "out_path = \"/content/imdb_pred.csv\"\n",
        "df_all.to_csv(out_path, index=False)\n",
        "\n",
        "print(\"saved:\", out_path)\n",
        "df_all.head(5)"
      ],
      "metadata": {
        "id": "9VZjkizV11Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## validation"
      ],
      "metadata": {
        "id": "eXiKmqLzhtCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def get_valid_preds_and_labels():\n",
        "    model.eval()\n",
        "    preds_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for batch in valid_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].cpu().numpy()\n",
        "\n",
        "        logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "\n",
        "        preds_list.append(preds)\n",
        "        labels_list.append(labels)\n",
        "\n",
        "    return np.concatenate(preds_list), np.concatenate(labels_list)\n"
      ],
      "metadata": {
        "id": "Y6zFprBEnG54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "preds, labels = get_valid_preds_and_labels()\n",
        "\n",
        "\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(labels, preds)\n",
        "cm_df = pd.DataFrame(\n",
        "    cm,\n",
        "    index=[\"true_neg\", \"true_pos\"],\n",
        "    columns=[\"pred_neg\", \"pred_pos\"]\n",
        ")\n",
        "print(\"confusion matrix\")\n",
        "print(cm_df)\n",
        "\n",
        "# classification report\n",
        "print(\"\\nclassification report\")\n",
        "print(classification_report(labels, preds, target_names=[\"negative\", \"positive\"], digits=4))\n"
      ],
      "metadata": {
        "id": "PWMNLmTXDM5E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}